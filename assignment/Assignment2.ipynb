{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebb6e6a",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d1ffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch questions tagged [nlp] from 2008 to 2025\n",
      "\n",
      "Fetching questions for year 2008...\n",
      "  Fetching page 1 for year 2008 (Attempt 1)...\n",
      "  Collected 45 new items from page 1. Total unique items: 45\n",
      "  No more pages for year 2008.\n",
      "Finished fetching for year 2008. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2009...\n",
      "  Fetching page 1 for year 2009 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 145\n",
      "  Fetching page 2 for year 2009 (Attempt 1)...\n",
      "  Collected 61 new items from page 2. Total unique items: 206\n",
      "  No more pages for year 2009.\n",
      "Finished fetching for year 2009. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2010...\n",
      "  Fetching page 1 for year 2010 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 306\n",
      "  Fetching page 2 for year 2010 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 406\n",
      "  Fetching page 3 for year 2010 (Attempt 1)...\n",
      "  Collected 99 new items from page 3. Total unique items: 505\n",
      "  No more pages for year 2010.\n",
      "Finished fetching for year 2010. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2011...\n",
      "  Fetching page 1 for year 2011 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 605\n",
      "  Fetching page 2 for year 2011 (Attempt 1)...\n",
      "  Collected 99 new items from page 2. Total unique items: 704\n",
      "  No more pages for year 2011.\n",
      "Finished fetching for year 2011. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2012...\n",
      "  Fetching page 1 for year 2012 (Attempt 1)...\n",
      "  Collected 99 new items from page 1. Total unique items: 803\n",
      "  No more pages for year 2012.\n",
      "Finished fetching for year 2012. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2013...\n",
      "  Fetching page 1 for year 2013 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 903\n",
      "  Fetching page 2 for year 2013 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 1003\n",
      "  Fetching page 3 for year 2013 (Attempt 1)...\n",
      "  Collected 99 new items from page 3. Total unique items: 1102\n",
      "  No more pages for year 2013.\n",
      "Finished fetching for year 2013. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2014...\n",
      "  Fetching page 1 for year 2014 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 1202\n",
      "  No more pages for year 2014.\n",
      "Finished fetching for year 2014. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2015...\n",
      "  Fetching page 1 for year 2015 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 1302\n",
      "  Fetching page 2 for year 2015 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 1402\n",
      "  Fetching page 3 for year 2015 (Attempt 1)...\n",
      "  Collected 98 new items from page 3. Total unique items: 1500\n",
      "  No more pages for year 2015.\n",
      "Finished fetching for year 2015. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2016...\n",
      "  Fetching page 1 for year 2016 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 1600\n",
      "  Fetching page 2 for year 2016 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 1700\n",
      "  Fetching page 3 for year 2016 (Attempt 1)...\n",
      "  Collected 100 new items from page 3. Total unique items: 1800\n",
      "  No more pages for year 2016.\n",
      "Finished fetching for year 2016. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2017...\n",
      "  Fetching page 1 for year 2017 (Attempt 1)...\n",
      "  Collected 99 new items from page 1. Total unique items: 1899\n",
      "  No more pages for year 2017.\n",
      "Finished fetching for year 2017. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2018...\n",
      "  Fetching page 1 for year 2018 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 1999\n",
      "  Fetching page 2 for year 2018 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 2099\n",
      "  Fetching page 3 for year 2018 (Attempt 1)...\n",
      "  Collected 100 new items from page 3. Total unique items: 2199\n",
      "  Fetching page 4 for year 2018 (Attempt 1)...\n",
      "  Collected 100 new items from page 4. Total unique items: 2299\n",
      "  Fetching page 5 for year 2018 (Attempt 1)...\n",
      "  Collected 100 new items from page 5. Total unique items: 2399\n",
      "  Fetching page 6 for year 2018 (Attempt 1)...\n",
      "  Collected 100 new items from page 6. Total unique items: 2499\n",
      "  No more pages for year 2018.\n",
      "Finished fetching for year 2018. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2019...\n",
      "  Fetching page 1 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 2599\n",
      "  Fetching page 2 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 2699\n",
      "  Fetching page 3 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 3. Total unique items: 2799\n",
      "  Fetching page 4 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 4. Total unique items: 2899\n",
      "  Fetching page 5 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 5. Total unique items: 2999\n",
      "  Fetching page 6 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 6. Total unique items: 3099\n",
      "  Fetching page 7 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 7. Total unique items: 3199\n",
      "  Fetching page 8 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 8. Total unique items: 3299\n",
      "  Fetching page 9 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 9. Total unique items: 3399\n",
      "  Fetching page 10 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 10. Total unique items: 3499\n",
      "  Fetching page 11 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 11. Total unique items: 3599\n",
      "  Fetching page 12 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 12. Total unique items: 3699\n",
      "  Fetching page 13 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 13. Total unique items: 3799\n",
      "  Fetching page 14 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 14. Total unique items: 3899\n",
      "  Fetching page 15 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 15. Total unique items: 3999\n",
      "  Fetching page 16 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 16. Total unique items: 4099\n",
      "  Fetching page 17 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 17. Total unique items: 4199\n",
      "  Fetching page 18 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 18. Total unique items: 4299\n",
      "  Fetching page 19 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 19. Total unique items: 4399\n",
      "  Fetching page 20 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 20. Total unique items: 4499\n",
      "  Fetching page 21 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 21. Total unique items: 4599\n",
      "  Fetching page 22 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 22. Total unique items: 4699\n",
      "  Fetching page 23 for year 2019 (Attempt 1)...\n",
      "  Collected 100 new items from page 23. Total unique items: 4799\n",
      "  Fetching page 24 for year 2019 (Attempt 1)...\n",
      "  Collected 44 new items from page 24. Total unique items: 4843\n",
      "  No more pages for year 2019.\n",
      "Finished fetching for year 2019. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2020...\n",
      "  Fetching page 1 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 4943\n",
      "  Fetching page 2 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 5043\n",
      "  Fetching page 3 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 3. Total unique items: 5143\n",
      "  Fetching page 4 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 4. Total unique items: 5243\n",
      "  Fetching page 5 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 5. Total unique items: 5343\n",
      "  Fetching page 6 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 6. Total unique items: 5443\n",
      "  Fetching page 7 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 7. Total unique items: 5543\n",
      "  Fetching page 8 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 8. Total unique items: 5643\n",
      "  Fetching page 9 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 9. Total unique items: 5743\n",
      "  Fetching page 10 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 10. Total unique items: 5843\n",
      "  Fetching page 11 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 11. Total unique items: 5943\n",
      "  Fetching page 12 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 12. Total unique items: 6043\n",
      "  Fetching page 13 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 13. Total unique items: 6143\n",
      "  Fetching page 14 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 14. Total unique items: 6243\n",
      "  Fetching page 15 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 15. Total unique items: 6343\n",
      "  Fetching page 16 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 16. Total unique items: 6443\n",
      "  Fetching page 17 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 17. Total unique items: 6543\n",
      "  Fetching page 18 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 18. Total unique items: 6643\n",
      "  Fetching page 19 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 19. Total unique items: 6743\n",
      "  Fetching page 20 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 20. Total unique items: 6843\n",
      "  Fetching page 21 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 21. Total unique items: 6943\n",
      "  Fetching page 22 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 22. Total unique items: 7043\n",
      "  Fetching page 23 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 23. Total unique items: 7143\n",
      "  Fetching page 24 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 24. Total unique items: 7243\n",
      "  Fetching page 25 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 25. Total unique items: 7343\n",
      "  Fetching page 26 for year 2020 (Attempt 1)...\n",
      "  Collected 100 new items from page 26. Total unique items: 7443\n",
      "  Fetching page 27 for year 2020 (Attempt 1)...\n",
      "  Collected 43 new items from page 27. Total unique items: 7486\n",
      "  No more pages for year 2020.\n",
      "Finished fetching for year 2020. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2021...\n",
      "  Fetching page 1 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 7586\n",
      "  Fetching page 2 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 7686\n",
      "  Fetching page 3 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 3. Total unique items: 7786\n",
      "  Fetching page 4 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 4. Total unique items: 7886\n",
      "  Fetching page 5 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 5. Total unique items: 7986\n",
      "  Fetching page 6 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 6. Total unique items: 8086\n",
      "  Fetching page 7 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 7. Total unique items: 8186\n",
      "  Fetching page 8 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 8. Total unique items: 8286\n",
      "  Fetching page 9 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 9. Total unique items: 8386\n",
      "  Fetching page 10 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 10. Total unique items: 8486\n",
      "  Fetching page 11 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 11. Total unique items: 8586\n",
      "  Fetching page 12 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 12. Total unique items: 8686\n",
      "  Fetching page 13 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 13. Total unique items: 8786\n",
      "  Fetching page 14 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 14. Total unique items: 8886\n",
      "  Fetching page 15 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 15. Total unique items: 8986\n",
      "  Fetching page 16 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 16. Total unique items: 9086\n",
      "  Fetching page 17 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 17. Total unique items: 9186\n",
      "  Fetching page 18 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 18. Total unique items: 9286\n",
      "  Fetching page 19 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 19. Total unique items: 9386\n",
      "  Fetching page 20 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 20. Total unique items: 9486\n",
      "  Fetching page 21 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 21. Total unique items: 9586\n",
      "  Fetching page 22 for year 2021 (Attempt 1)...\n",
      "  Collected 100 new items from page 22. Total unique items: 9686\n",
      "  Fetching page 23 for year 2021 (Attempt 1)...\n",
      "  Collected 32 new items from page 23. Total unique items: 9718\n",
      "  No more pages for year 2021.\n",
      "Finished fetching for year 2021. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2022...\n",
      "  Fetching page 1 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 9818\n",
      "  Fetching page 2 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 9918\n",
      "  Fetching page 3 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 3. Total unique items: 10018\n",
      "  Fetching page 4 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 4. Total unique items: 10118\n",
      "  Fetching page 5 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 5. Total unique items: 10218\n",
      "  Fetching page 6 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 6. Total unique items: 10318\n",
      "  Fetching page 7 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 7. Total unique items: 10418\n",
      "  Fetching page 8 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 8. Total unique items: 10518\n",
      "  Fetching page 9 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 9. Total unique items: 10618\n",
      "  Fetching page 10 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 10. Total unique items: 10718\n",
      "  Fetching page 11 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 11. Total unique items: 10818\n",
      "  Fetching page 12 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 12. Total unique items: 10918\n",
      "  Fetching page 13 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 13. Total unique items: 11018\n",
      "  Fetching page 14 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 14. Total unique items: 11118\n",
      "  Fetching page 15 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 15. Total unique items: 11218\n",
      "  Fetching page 16 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 16. Total unique items: 11318\n",
      "  Fetching page 17 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 17. Total unique items: 11418\n",
      "  Fetching page 18 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 18. Total unique items: 11518\n",
      "  Fetching page 19 for year 2022 (Attempt 1)...\n",
      "  Collected 100 new items from page 19. Total unique items: 11618\n",
      "  Fetching page 20 for year 2022 (Attempt 1)...\n",
      "  Collected 49 new items from page 20. Total unique items: 11667\n",
      "  No more pages for year 2022.\n",
      "Finished fetching for year 2022. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2023...\n",
      "  Fetching page 1 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 11767\n",
      "  Fetching page 2 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 11867\n",
      "  Fetching page 3 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 3. Total unique items: 11967\n",
      "  Fetching page 4 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 4. Total unique items: 12067\n",
      "  Fetching page 5 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 5. Total unique items: 12167\n",
      "  Fetching page 6 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 6. Total unique items: 12267\n",
      "  Fetching page 7 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 7. Total unique items: 12367\n",
      "  Fetching page 8 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 8. Total unique items: 12467\n",
      "  Fetching page 9 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 9. Total unique items: 12567\n",
      "  Fetching page 10 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 10. Total unique items: 12667\n",
      "  Fetching page 11 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 11. Total unique items: 12767\n",
      "  Fetching page 12 for year 2023 (Attempt 1)...\n",
      "  Collected 100 new items from page 12. Total unique items: 12867\n",
      "  Fetching page 13 for year 2023 (Attempt 1)...\n",
      "  Collected 55 new items from page 13. Total unique items: 12922\n",
      "  No more pages for year 2023.\n",
      "Finished fetching for year 2023. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2024...\n",
      "  Fetching page 1 for year 2024 (Attempt 1)...\n",
      "  Collected 100 new items from page 1. Total unique items: 13022\n",
      "  Fetching page 2 for year 2024 (Attempt 1)...\n",
      "  Collected 100 new items from page 2. Total unique items: 13122\n",
      "  Fetching page 3 for year 2024 (Attempt 1)...\n",
      "  Collected 100 new items from page 3. Total unique items: 13222\n",
      "  Fetching page 4 for year 2024 (Attempt 1)...\n",
      "  Collected 100 new items from page 4. Total unique items: 13322\n",
      "  Fetching page 5 for year 2024 (Attempt 1)...\n",
      "  Collected 100 new items from page 5. Total unique items: 13422\n",
      "  Fetching page 6 for year 2024 (Attempt 1)...\n",
      "  Collected 100 new items from page 6. Total unique items: 13522\n",
      "  Fetching page 7 for year 2024 (Attempt 1)...\n",
      "  Collected 53 new items from page 7. Total unique items: 13575\n",
      "  No more pages for year 2024.\n",
      "Finished fetching for year 2024. Pausing briefly...\n",
      "\n",
      "Fetching questions for year 2025...\n",
      "  Fetching page 1 for year 2025 (Attempt 1)...\n",
      "  Collected 74 new items from page 1. Total unique items: 13649\n",
      "  No more pages for year 2025.\n",
      "Finished fetching for year 2025. Pausing briefly...\n",
      "\n",
      "Finished fetching questions. Total unique questions collected: 13649\n",
      "\n",
      "Total unique questions collected: 13649\n",
      "\n",
      "********************************************************************\n",
      "Warning: Collected 13649 posts, which is less than the target of 20,000.\n",
      "This might be due to API limitations, daily quotas (even with a key),\n",
      "or the actual number of posts available for the tag 'nlp'.\n",
      "Ensure the API Key used is valid and has sufficient quota.\n",
      "Document this limitation if the target cannot be reached.\n",
      "********************************************************************\n",
      "\n",
      "Found 5358 unique accepted answer IDs to fetch.\n",
      "\n",
      "Starting to fetch bodies for 5358 accepted answers...\n",
      "  Fetching batch 1/54 (IDs: 56754191...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 2/54 (IDs: 2589290...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 3/54 (IDs: 3114191...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 4/54 (IDs: 66815715...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 5/54 (IDs: 3705061...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 6/54 (IDs: 7310173...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 7/54 (IDs: 19959203...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 8/54 (IDs: 58036244...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 9/54 (IDs: 63509069...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 10/54 (IDs: 34444486...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 11/54 (IDs: 65705829...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 12/54 (IDs: 40245778...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 13/54 (IDs: 64494663...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 14/54 (IDs: 70131406...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 15/54 (IDs: 72982833...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 16/54 (IDs: 71803813...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 17/54 (IDs: 76293658...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 18/54 (IDs: 56010862...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 19/54 (IDs: 58469168...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 20/54 (IDs: 71118272...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 21/54 (IDs: 77017166...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 22/54 (IDs: 67973798...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 23/54 (IDs: 72758507...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 24/54 (IDs: 52705199...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 25/54 (IDs: 62110273...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 26/54 (IDs: 68140267...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 27/54 (IDs: 67616524...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 28/54 (IDs: 66306305...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 29/54 (IDs: 69124935...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 30/54 (IDs: 49989092...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 31/54 (IDs: 58148826...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 32/54 (IDs: 64866904...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 33/54 (IDs: 63130831...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 34/54 (IDs: 66604855...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 35/54 (IDs: 52810148...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 36/54 (IDs: 57496598...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 37/54 (IDs: 8017470...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 38/54 (IDs: 55761147...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 39/54 (IDs: 48028439...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 40/54 (IDs: 74440050...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 41/54 (IDs: 72736695...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 42/54 (IDs: 53371517...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 43/54 (IDs: 20604052...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 44/54 (IDs: 53798635...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 45/54 (IDs: 70347017...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 46/54 (IDs: 66087770...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 47/54 (IDs: 61959640...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 48/54 (IDs: 40628283...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 49/54 (IDs: 60387996...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 50/54 (IDs: 57374024...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 51/54 (IDs: 76314471...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 52/54 (IDs: 61208992...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 53/54 (IDs: 71662595...) (Attempt 1)\n",
      "  Fetched 100 answer bodies in this batch.\n",
      "  Fetching batch 54/54 (IDs: 77069904...) (Attempt 1)\n",
      "  Fetched 58 answer bodies in this batch.\n",
      "\n",
      "Finished fetching answer bodies. Found bodies for 5358 answers.\n",
      "\n",
      "Building final DataFrame...\n",
      "\n",
      "Final DataFrame structure:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13649 entries, 0 to 13648\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   question_id           13649 non-null  int64         \n",
      " 1   title                 13649 non-null  object        \n",
      " 2   body                  13649 non-null  object        \n",
      " 3   tags                  13649 non-null  object        \n",
      " 4   accepted_answer_id    5358 non-null   float64       \n",
      " 5   accepted_answer_body  5358 non-null   object        \n",
      " 6   creation_date         13649 non-null  datetime64[ns]\n",
      " 7   view_count            13649 non-null  int64         \n",
      " 8   link                  13649 non-null  object        \n",
      " 9   is_answered           13649 non-null  bool          \n",
      "dtypes: bool(1), datetime64[ns](1), float64(1), int64(2), object(5)\n",
      "memory usage: 973.2+ KB\n",
      "None\n",
      "\n",
      "Sample data:\n",
      "   question_id                                              title  \\\n",
      "0       402440  About \"AUTOMATIC TEXT SUMMARIZER (lingustic ba...   \n",
      "1       395421  Is there open source software available that a...   \n",
      "2       393248  Counting the number of occurrences of words in...   \n",
      "3       348958  Algorithms recognizing physical address on a w...   \n",
      "4       341455  End user tool for generating a regular expression   \n",
      "\n",
      "                                                body  \\\n",
      "0  <p>I am having \"AUTOMATIC TEXT SUMMARIZER (lin...   \n",
      "1  <p>I can't find anything other than closed-sou...   \n",
      "2  <p>How could I go about keeping track of the n...   \n",
      "3  <p>What are the best algorithms for recognizin...   \n",
      "4  <p>We have a SaaS application requirement to a...   \n",
      "\n",
      "                                                tags  accepted_answer_id  \\\n",
      "0            [text, nlp, linguistics, summarization]                 NaN   \n",
      "1                             [string, file-io, nlp]                 NaN   \n",
      "2                      [c, algorithm, nlp, counting]            393265.0   \n",
      "3  [algorithm, screen-scraping, nlp, pattern-matc...            349300.0   \n",
      "4                                 [.net, regex, nlp]                 NaN   \n",
      "\n",
      "                                accepted_answer_body       creation_date  \\\n",
      "0                                                NaN 2008-12-31 06:56:57   \n",
      "1                                                NaN 2008-12-27 21:20:20   \n",
      "2  <p>Yes, a dictionary with word-occurence pairs... 2008-12-25 22:34:51   \n",
      "3  <p>A named-entity extraction framework such as... 2008-12-08 09:06:36   \n",
      "4                                                NaN 2008-12-04 17:35:31   \n",
      "\n",
      "   view_count                                               link  is_answered  \n",
      "0        3443  https://stackoverflow.com/questions/402440/abo...         True  \n",
      "1         613  https://stackoverflow.com/questions/395421/is-...         True  \n",
      "2       17093  https://stackoverflow.com/questions/393248/cou...         True  \n",
      "3        4569  https://stackoverflow.com/questions/348958/alg...         True  \n",
      "4         570  https://stackoverflow.com/questions/341455/end...         True  \n",
      "\n",
      "Successfully saved collected data to nlp_stackoverflow_posts_collected.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import html # for decoding HTML entities\n",
    "\n",
    "STACK_API_KEY = 'rl_dcgdAD45Dx2PKtP3xdw9FFoB2' \n",
    "# -------------------------------------------------------------\n",
    "\n",
    "if STACK_API_KEY is None: \n",
    "    print(\"*\"*60)\n",
    "    print(\"Warning: STACK_API_KEY not configured.\")\n",
    "    print(\"Strongly recommend registering and using an API Key for higher request quota.\")\n",
    "    print(\"Visit https://stackapps.com/apps/oauth/register to get a Key.\")\n",
    "    print(\"Running without a Key can easily trigger 429 Too Many Requests errors.\")\n",
    "    print(\"*\"*60)\n",
    "    # time.sleep(5)\n",
    "\n",
    "TAG_TO_FETCH = 'nlp'\n",
    "START_YEAR = 2008\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "OUTPUT_FILENAME = f\"{TAG_TO_FETCH}_stackoverflow_posts_collected.csv\"\n",
    "API_BASE_URL = \"https://api.stackexchange.com/2.3\"\n",
    "API_SITE = \"stackoverflow\"\n",
    "MAX_RETRIES_ON_429 = 3 \n",
    "INITIAL_WAIT_ON_429 = 60 \n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "def decode_html_entities(text):\n",
    "    \"\"\"Decode HTML entities in text, e.g., &amp; -> &\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return html.unescape(text)\n",
    "    return text\n",
    "\n",
    "# --- Main data retrieval functions ---\n",
    "\n",
    "def fetch_questions(tag, start_year, end_year, api_key=None):\n",
    "    \"\"\"Get question data for specified tag and year range, with retry logic\"\"\"\n",
    "    all_questions = []\n",
    "    question_ids = set()\n",
    "    question_filter = 'withbody' \n",
    "\n",
    "    print(f\"Starting to fetch questions tagged [{tag}] from {start_year} to {end_year}\")\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"\\nFetching questions for year {year}...\")\n",
    "        from_timestamp = int(datetime(year, 1, 1).timestamp())\n",
    "        to_timestamp = int(datetime(year, 12, 31, 23, 59, 59).timestamp())\n",
    "\n",
    "        page = 1\n",
    "        has_more = True\n",
    "        retries_429 = 0 \n",
    "        current_wait_429 = INITIAL_WAIT_ON_429\n",
    "\n",
    "        while has_more:\n",
    "            print(f\"  Fetching page {page} for year {year} (Attempt {retries_429 + 1})...\")\n",
    "            params = {\n",
    "                'order': 'desc', 'sort': 'creation', 'tagged': tag,\n",
    "                'site': API_SITE, 'pagesize': 100, 'page': page,\n",
    "                'filter': question_filter, 'fromdate': from_timestamp,\n",
    "                'todate': to_timestamp\n",
    "            }\n",
    "            if api_key:\n",
    "                params['key'] = api_key\n",
    "\n",
    "            data = None \n",
    "            try:\n",
    "                response = requests.get(f\"{API_BASE_URL}/questions\", params=params, timeout=45) # Longer timeout\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    retries_429 = 0 \n",
    "                    current_wait_429 = INITIAL_WAIT_ON_429 \n",
    "                    data = response.json()\n",
    "\n",
    "                    if 'backoff' in data:\n",
    "                        wait_time = data['backoff']\n",
    "                        print(f\"  API requested backoff: Sleeping for {wait_time} seconds.\")\n",
    "                        time.sleep(wait_time + 1)\n",
    "                    else:\n",
    "                        time.sleep(1.1) \n",
    "\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"  Error: Received status code 429 (Too Many Requests).\")\n",
    "                    if retries_429 < MAX_RETRIES_ON_429:\n",
    "                        retries_429 += 1\n",
    "                        print(f\"  Waiting for {current_wait_429} seconds before retry ({retries_429}/{MAX_RETRIES_ON_429})...\")\n",
    "                        time.sleep(current_wait_429)\n",
    "                        current_wait_429 *= 2 \n",
    "                        continue \n",
    "                    else:\n",
    "                        print(f\"  Max retries ({MAX_RETRIES_ON_429}) reached for page {page}, year {year}. Stopping for this year.\")\n",
    "                        has_more = False \n",
    "                        data = None\n",
    "                else:\n",
    "                    print(f\"  Error: Received status code {response.status_code}\")\n",
    "                    print(f\"  Response content: {response.text[:500]}\")\n",
    "                    print(f\"  Stopping fetch for year {year}.\")\n",
    "                    has_more = False \n",
    "                    data = None\n",
    "                    time.sleep(5) \n",
    "\n",
    "                # --- Process successfully retrieved data ---\n",
    "                if data is not None and has_more:\n",
    "                    items = data.get('items', [])\n",
    "                    if not items:\n",
    "                        print(f\"  No more items found for year {year} on page {page}.\")\n",
    "                        has_more = False\n",
    "                    else:\n",
    "                        new_items_count = 0\n",
    "                        for item in items:\n",
    "                            if item['question_id'] not in question_ids:\n",
    "                               item['title'] = decode_html_entities(item.get('title'))\n",
    "                               item['body'] = decode_html_entities(item.get('body'))\n",
    "                               question_data = {\n",
    "                                   'question_id': item.get('question_id'), 'title': item.get('title'),\n",
    "                                   'body': item.get('body'), 'tags': item.get('tags', []),\n",
    "                                   'accepted_answer_id': item.get('accepted_answer_id'),\n",
    "                                   'creation_date': item.get('creation_date'),\n",
    "                                   'view_count': item.get('view_count', 0), 'link': item.get('link'),\n",
    "                                   'is_answered': item.get('is_answered', False)\n",
    "                               }\n",
    "                               all_questions.append(question_data)\n",
    "                               question_ids.add(item['question_id'])\n",
    "                               new_items_count += 1\n",
    "\n",
    "                        print(f\"  Collected {new_items_count} new items from page {page}. Total unique items: {len(all_questions)}\")\n",
    "                        has_more = data.get('has_more', False)\n",
    "                        page += 1\n",
    "\n",
    "                    if not has_more:\n",
    "                         print(f\"  No more pages for year {year}.\")\n",
    "\n",
    "                    if data.get('quota_remaining', 1) <= 1:\n",
    "                        print(\"  Warning: Low API quota remaining. Sleeping for 60 seconds.\")\n",
    "                        time.sleep(60)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  Network error fetching page {page} for year {year}: {e}\")\n",
    "                print(\"  Waiting for 60 seconds before potentially retrying...\")\n",
    "                time.sleep(60)\n",
    "                # Simple handling: can choose to retry (continue) or abandon current year (break/has_more=False)\n",
    "                # Here we choose to abandon the current year to simplify logic\n",
    "                print(f\"  Stopping fetch for year {year} due to network error.\")\n",
    "                has_more = False\n",
    "\n",
    "        # Brief pause after completing a year\n",
    "        print(f\"Finished fetching for year {year}. Pausing briefly...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    print(f\"\\nFinished fetching questions. Total unique questions collected: {len(all_questions)}\")\n",
    "    return all_questions\n",
    "\n",
    "\n",
    "def fetch_answer_bodies(answer_ids, api_key=None):\n",
    "    \"\"\"Batch retrieve answer bodies based on answer ID list, with retry logic\"\"\"\n",
    "    print(f\"\\nStarting to fetch bodies for {len(answer_ids)} accepted answers...\")\n",
    "    answer_bodies = {}\n",
    "    ids_to_fetch = list(answer_ids)\n",
    "    answer_filter = 'withbody'\n",
    "    batch_size = 100\n",
    "\n",
    "    for i in range(0, len(ids_to_fetch), batch_size):\n",
    "        chunk = ids_to_fetch[i : i + batch_size]\n",
    "        ids_str = \";\".join(map(str, chunk))\n",
    "        retries_429 = 0\n",
    "        current_wait_429 = INITIAL_WAIT_ON_429\n",
    "        current_batch_success = False\n",
    "\n",
    "        while retries_429 <= MAX_RETRIES_ON_429 and not current_batch_success:\n",
    "            print(f\"  Fetching batch {i // batch_size + 1}/{(len(ids_to_fetch) + batch_size - 1) // batch_size} (IDs: {chunk[0]}...) (Attempt {retries_429 + 1})\")\n",
    "            params = {\n",
    "                'site': API_SITE, 'filter': answer_filter,\n",
    "                'pagesize': batch_size\n",
    "            }\n",
    "            if api_key:\n",
    "                params['key'] = api_key\n",
    "\n",
    "            data = None\n",
    "            try:\n",
    "                response = requests.get(f\"{API_BASE_URL}/answers/{ids_str}\", params=params, timeout=45)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    current_batch_success = True \n",
    "                    data = response.json()\n",
    "                    if 'backoff' in data:\n",
    "                        wait_time = data['backoff']\n",
    "                        print(f\"  API requested backoff: Sleeping for {wait_time} seconds.\")\n",
    "                        time.sleep(wait_time + 1)\n",
    "                    else:\n",
    "                        time.sleep(1.1)\n",
    "\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"  Error: Received status code 429 (Too Many Requests).\")\n",
    "                    retries_429 += 1\n",
    "                    if retries_429 <= MAX_RETRIES_ON_429:\n",
    "                        print(f\"  Waiting for {current_wait_429} seconds before retry ({retries_429}/{MAX_RETRIES_ON_429})...\")\n",
    "                        time.sleep(current_wait_429)\n",
    "                        current_wait_429 *= 2\n",
    "                        # No need for continue, the while condition will handle retries\n",
    "                    else:\n",
    "                        print(f\"  Max retries ({MAX_RETRIES_ON_429}) reached for answer batch starting with ID {chunk[0]}. Skipping batch.\")\n",
    "                        # Break out of inner while loop, process next batch\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"  Error: Received status code {response.status_code} fetching answers.\")\n",
    "                    print(f\"  Response content: {response.text[:500]}\")\n",
    "                    print(f\"  Skipping this batch of answers.\")\n",
    "                    # Break out of inner while loop\n",
    "                    break\n",
    "\n",
    "                # --- Process successfully retrieved data ---\n",
    "                if data is not None and current_batch_success:\n",
    "                    items = data.get('items', [])\n",
    "                    if items:\n",
    "                        fetched_count = 0\n",
    "                        for item in items:\n",
    "                            answer_bodies[item['answer_id']] = decode_html_entities(item.get('body'))\n",
    "                            fetched_count +=1\n",
    "                        print(f\"  Fetched {fetched_count} answer bodies in this batch.\")\n",
    "                    else:\n",
    "                        print(\"  No items returned for this batch of answer IDs.\")\n",
    "\n",
    "                    if data.get('quota_remaining', 1) <= 1:\n",
    "                        print(\"  Warning: Low API quota remaining. Sleeping for 60 seconds.\")\n",
    "                        time.sleep(60)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  Network error fetching answer batch starting with ID {chunk[0]}: {e}\")\n",
    "                print(\"  Waiting for 60 seconds before potentially retrying...\")\n",
    "                time.sleep(60)\n",
    "                # Simple handling: retry (if retries_429 not at limit) or abandon\n",
    "                retries_429 += 1\n",
    "                if retries_429 > MAX_RETRIES_ON_429:\n",
    "                    print(f\"  Skipping batch due to repeated network errors.\")\n",
    "                    break # Break out of inner while loop\n",
    "\n",
    "    print(f\"\\nFinished fetching answer bodies. Found bodies for {len(answer_bodies)} answers.\")\n",
    "    return answer_bodies\n",
    "\n",
    "\n",
    "# --- Main program ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Get all questions\n",
    "    questions = fetch_questions(TAG_TO_FETCH, START_YEAR, CURRENT_YEAR, STACK_API_KEY)\n",
    "\n",
    "    if not questions:\n",
    "        print(\"No questions collected. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"\\nTotal unique questions collected: {len(questions)}\")\n",
    "    if len(questions) < 20000:\n",
    "        print(f\"Warning: Collected {len(questions)} posts, which is less than the target of 20,000.\")\n",
    "\n",
    "\n",
    "    # 2. Extract accepted answer IDs that need body fetching\n",
    "    accepted_answer_ids = set(q['accepted_answer_id'] for q in questions if q.get('accepted_answer_id'))\n",
    "    print(f\"Found {len(accepted_answer_ids)} unique accepted answer IDs to fetch.\")\n",
    "\n",
    "    # 3. Get answer bodies\n",
    "    answer_bodies_map = fetch_answer_bodies(accepted_answer_ids, STACK_API_KEY) if accepted_answer_ids else {}\n",
    "\n",
    "    # 4. Build DataFrame\n",
    "    print(\"\\nBuilding final DataFrame...\")\n",
    "    df = pd.DataFrame(questions)\n",
    "    df['creation_date'] = pd.to_datetime(df['creation_date'], unit='s')\n",
    "    df['accepted_answer_body'] = df['accepted_answer_id'].map(answer_bodies_map)\n",
    "\n",
    "    missing_bodies_count = df['accepted_answer_id'].notna().sum() - df['accepted_answer_body'].notna().sum()\n",
    "    if missing_bodies_count > 0:\n",
    "         print(f\"Info: Could not fetch bodies for {missing_bodies_count} accepted answers (may be deleted/inaccessible).\")\n",
    "\n",
    "    # 5. Select and organize final columns\n",
    "    final_columns = [\n",
    "        'question_id', 'title', 'body', 'tags', 'accepted_answer_id',\n",
    "        'accepted_answer_body', 'creation_date', 'view_count', 'link', 'is_answered'\n",
    "    ]\n",
    "    final_columns = [col for col in final_columns if col in df.columns]\n",
    "    df_final = df[final_columns]\n",
    "\n",
    "    print(\"\\nFinal DataFrame structure:\")\n",
    "    print(df_final.info())\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df_final.head())\n",
    "\n",
    "    # 6. Save to CSV\n",
    "    try:\n",
    "        df_final.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8')\n",
    "        print(f\"\\nSuccessfully saved collected data to {OUTPUT_FILENAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving data to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ee037",
   "metadata": {},
   "source": [
    "## 2. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8bbd8d-f1ef-4abb-b554-3180b1af99ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d70c76d-62a5-4099-b288-f615c843e7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "Original dataset contains 13649 rows and 10 columns\n",
      "Performing HTML decoding...\n",
      "Removing HTML tags...\n",
      "Converting to lowercase...\n",
      "Removing punctuation and special characters...\n",
      "Performing tokenization and stopword removal...\n",
      "Processing tags column...\n",
      "Data preprocessing complete!\n",
      "Preprocessed data saved to 'nlp_stackoverflow_posts_preprocessed.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import html\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_data(csv_file):\n",
    "    \"\"\"\n",
    "    Perform preprocessing on the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Preprocessed dataframe\n",
    "    \"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Original dataset contains {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    \n",
    "    # 1. HTML decoding - decode HTML entities (e.g., &amp; -> &)\n",
    "    print(\"Performing HTML decoding...\")\n",
    "    for col in ['title', 'body', 'accepted_answer_body']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # 2. Remove HTML tags - remove <p>, <code>, etc. tags\n",
    "    print(\"Removing HTML tags...\")\n",
    "    def remove_html_tags(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        return re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    for col in ['title', 'body', 'accepted_answer_body']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(remove_html_tags)\n",
    "    \n",
    "    # 3. Convert to lowercase\n",
    "    print(\"Converting to lowercase...\")\n",
    "    for col in ['title', 'body', 'accepted_answer_body']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    \n",
    "    # 4. Remove punctuation and special characters\n",
    "    print(\"Removing punctuation and special characters...\")\n",
    "    def remove_punctuation(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        # Create a translation table to map all punctuation to spaces\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        # Apply the translation table\n",
    "        return text.translate(translator)\n",
    "    \n",
    "    for col in ['title', 'body', 'accepted_answer_body']:\n",
    "        if col in df.columns:\n",
    "            df[col + '_clean'] = df[col].apply(remove_punctuation)\n",
    "    \n",
    "    # 5. Tokenization and stopword removal\n",
    "    print(\"Performing tokenization and stopword removal...\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def tokenize_and_remove_stopwords(text):\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return filtered_tokens\n",
    "    \n",
    "    for col in ['title', 'body', 'accepted_answer_body']:\n",
    "        if col in df.columns:\n",
    "            df[col + '_tokens'] = df[col + '_clean'].apply(tokenize_and_remove_stopwords)\n",
    "    \n",
    "    # 6. Process tags column - if tags is a string representation of a list, convert to actual list\n",
    "    print(\"Processing tags column...\")\n",
    "    if 'tags' in df.columns:\n",
    "        df['tags_list'] = df['tags'].apply(lambda x: eval(x) if isinstance(x, str) and x.startswith('[') else x)\n",
    "    \n",
    "    print(\"Data preprocessing complete!\")\n",
    "    return df\n",
    "\n",
    "# Use the function\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_df = preprocess_data(\"nlp_stackoverflow_posts_collected.csv\")\n",
    "    # Save the preprocessed data\n",
    "    preprocessed_df.to_csv(\"nlp_stackoverflow_posts_preprocessed.csv\", index=False)\n",
    "    print(\"Preprocessed data saved to 'nlp_stackoverflow_posts_preprocessed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c29a36",
   "metadata": {},
   "source": [
    "## 3.  Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45926ce7-0aa1-40ae-af25-776cef5ad3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting word cloud creation...\n",
      "Using 'title_tokens' column to generate word cloud...\n",
      "Word cloud saved to 'visualizations/nlp_title_wordcloud.png' and 'visualizations/nlp_title_wordcloud_plt.png'\n",
      "Creating frequency chart of the top 20 common words...\n",
      "Word frequency chart saved to 'visualizations/nlp_top_words_frequency.png'\n",
      "Word cloud visualization complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def create_wordcloud(csv_file, output_folder='visualizations'):\n",
    "    \"\"\"\n",
    "    Generate a word cloud for post titles\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the preprocessed CSV file\n",
    "    output_folder (str): Output folder\n",
    "    \"\"\"\n",
    "    print(\"Starting word cloud creation...\")\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Read the preprocessed data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    if 'title_tokens' in df.columns:\n",
    "        text_column = 'title_tokens'\n",
    "        # Join token lists into strings\n",
    "        text_data = df[text_column].apply(lambda x: ' '.join(eval(x)) if isinstance(x, str) else '').str.cat(sep=' ')\n",
    "    elif 'title_clean' in df.columns:\n",
    "        text_column = 'title_clean'\n",
    "        text_data = ' '.join(df[text_column].dropna().astype(str))\n",
    "    else:\n",
    "        text_column = 'title'\n",
    "        text_data = ' '.join(df[text_column].dropna().astype(str))\n",
    "    \n",
    "    print(f\"Using '{text_column}' column to generate word cloud...\")\n",
    "    \n",
    "    \n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        max_words=200,\n",
    "        # mask=mask,  # Uncomment to use custom shape\n",
    "        contour_width=1,\n",
    "        contour_color='steelblue',\n",
    "        colormap='viridis'\n",
    "    ).generate(text_data)\n",
    "    \n",
    "    # Save the word cloud image\n",
    "    wordcloud_path = os.path.join(output_folder, 'nlp_title_wordcloud.png')\n",
    "    wordcloud.to_file(wordcloud_path)\n",
    "    \n",
    "    # Display the word cloud using matplotlib\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('Common Terms in Stack Overflow NLP Post Titles', fontsize=20)\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    # Save the matplotlib image\n",
    "    plt_path = os.path.join(output_folder, 'nlp_title_wordcloud_plt.png')\n",
    "    plt.savefig(plt_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Word cloud saved to '{wordcloud_path}' and '{plt_path}'\")\n",
    "    \n",
    "    print(\"Creating frequency chart of the top 20 common words...\")\n",
    "    word_freq = {}\n",
    "    for tokens in df[text_column]:\n",
    "        if isinstance(tokens, str):\n",
    "            try:\n",
    "                token_list = eval(tokens) if tokens.startswith('[') else tokens.split()\n",
    "                for word in token_list:\n",
    "                    if len(word) > 2:  # Ignore words that are too short\n",
    "                        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Get the top 20 most common words\n",
    "    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    words = [item[0] for item in top_words]\n",
    "    freqs = [item[1] for item in top_words]\n",
    "    \n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(words, freqs, color='skyblue')\n",
    "    plt.xlabel('Frequency', fontsize=12)\n",
    "    plt.ylabel('Word', fontsize=12)\n",
    "    plt.title('Top 20 Most Common Terms in Stack Overflow NLP Posts', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the frequency chart\n",
    "    freq_path = os.path.join(output_folder, 'nlp_top_words_frequency.png')\n",
    "    plt.savefig(freq_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Word frequency chart saved to '{freq_path}'\")\n",
    "    print(\"Word cloud visualization complete!\")\n",
    "    \n",
    "    return wordcloud_path, freq_path\n",
    "\n",
    "# Use the function\n",
    "if __name__ == \"__main__\":\n",
    "    create_wordcloud(\"nlp_stackoverflow_posts_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415192e",
   "metadata": {},
   "source": [
    "## 4. Posts Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19f2d69-6b61-4fa2-9040-0dbf3dceea12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting post categorization...\n",
      "Read 13649 posts for categorization\n",
      "Processed 0 posts...\n",
      "Processed 1000 posts...\n",
      "Processed 2000 posts...\n",
      "Processed 3000 posts...\n",
      "Processed 4000 posts...\n",
      "Processed 5000 posts...\n",
      "Processed 6000 posts...\n",
      "Processed 7000 posts...\n",
      "Processed 8000 posts...\n",
      "Processed 9000 posts...\n",
      "Processed 10000 posts...\n",
      "Processed 11000 posts...\n",
      "Processed 12000 posts...\n",
      "Processed 13000 posts...\n",
      "Categorization complete! Categorized a total of 12833 posts\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "implementation_issues    6208\n",
      "nlp_task                 3851\n",
      "understanding_issues     2210\n",
      "nlp_technology            564\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Subcategory distribution:\n",
      "subcategory\n",
      "tokenization               3204\n",
      "text_similarity            2832\n",
      "entity_recognition         1407\n",
      "pos_tagging                1105\n",
      "stemming_lemmatization      686\n",
      "sentiment_analysis          442\n",
      "word_embeddings             355\n",
      "transformers                334\n",
      "spacy                       262\n",
      "nltk                        202\n",
      "topic_modeling              189\n",
      "text_summarization          185\n",
      "machine_translation         115\n",
      "tensorflow_keras             82\n",
      "stanford_nlp                 63\n",
      "pytorch                      60\n",
      "gensim                       37\n",
      "language_identification      25\n",
      "openai                        3\n",
      "Name: count, dtype: int64\n",
      "Categorization results saved to 'categorization/nlp_posts_categorized.csv'\n",
      "Classification rules saved to 'categorization/classification_rules.json'\n",
      "Post categorization complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "def categorize_posts(csv_file, output_folder='categorization'):\n",
    "    \"\"\"\n",
    "    Categorize NLP posts\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the preprocessed CSV file\n",
    "    output_folder (str): Output folder\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing categorization results\n",
    "    \"\"\"\n",
    "    print(\"Starting post categorization...\")\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Read the preprocessed data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Read {df.shape[0]} posts for categorization\")\n",
    "    \n",
    "    # Initialize categorization columns\n",
    "    df['category'] = None\n",
    "    df['subcategory'] = None\n",
    "    df['classification_reason'] = None\n",
    "    \n",
    "    # Define categorization rules\n",
    "    \n",
    "    # 1. Implementation Issues\n",
    "    implementation_patterns = [\n",
    "        r'\\bhow to\\b', r'\\bhow do i\\b', r'\\bhow can i\\b', r'\\bhow would i\\b',\n",
    "        r'\\bhow should i\\b', r'\\bimplementation\\b', r'\\bimplement\\b'\n",
    "    ]\n",
    "    \n",
    "    # 2. Understanding Issues\n",
    "    understanding_patterns = [\n",
    "        r'\\bwhat is\\b', r'\\bwhat are\\b', r'\\bwhy\\b', r'\\bwhat does\\b',\n",
    "        r'\\bmean\\b', r'\\bmeaning\\b', r'\\bdefinition\\b', r'\\bunderstand\\b', r'\\bexplain\\b'\n",
    "    ]\n",
    "    \n",
    "    # 3. Task-based categories\n",
    "    nlp_tasks = {\n",
    "        'text_similarity': [\n",
    "            r'similar', r'similarity', r'distance', r'compare text', r'match', \n",
    "            r'semantic similarity', r'document similarity', r'text comparison'\n",
    "        ],\n",
    "        'tokenization': [\n",
    "            r'tokeniz', r'token', r'split text', r'segment', r'word breaking',\n",
    "            r'sentence boundary', r'sentence splitting'\n",
    "        ],\n",
    "        'stemming_lemmatization': [\n",
    "            r'stem', r'lemma', r'root word', r'word form', r'inflect',\n",
    "            r'morphological', r'normalization'\n",
    "        ],\n",
    "        'language_identification': [\n",
    "            r'language detect', r'identify language', r'determine language',\n",
    "            r'language recognition', r'language identification'\n",
    "        ],\n",
    "        'sentiment_analysis': [\n",
    "            r'sentiment', r'opinion', r'emotion', r'feeling', r'polarity',\n",
    "            r'positive negative', r'attitude', r'tone'\n",
    "        ],\n",
    "        'topic_modeling': [\n",
    "            r'topic model', r'lda', r'latent dirichlet', r'topic extraction',\n",
    "            r'theme', r'document clustering', r'topic classification'\n",
    "        ],\n",
    "        'entity_recognition': [\n",
    "            r'ner', r'named entity', r'entity extraction', r'entity recognition',\n",
    "            r'entity detect', r'information extraction'\n",
    "        ],\n",
    "        'text_summarization': [\n",
    "            r'summar', r'extract key', r'condense', r'digest', r'abstract',\n",
    "            r'text reduction', r'key information'\n",
    "        ],\n",
    "        'pos_tagging': [\n",
    "            r'pos tag', r'part of speech', r'pos', r'syntactic category',\n",
    "            r'grammatical tag', r'word class'\n",
    "        ],\n",
    "        'machine_translation': [\n",
    "            r'translat', r'language conversion', r'cross-lingual', r'multilingual',\n",
    "            r'language transfer'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 4. Technology-based categories\n",
    "    technologies = {\n",
    "        'spacy': [r'spacy', r'spacyjs'],\n",
    "        'nltk': [r'nltk', r'natural language toolkit'],\n",
    "        'transformers': [\n",
    "            r'transformer', r'bert', r'gpt', r'huggingface', r't5', \n",
    "            r'roberta', r'xlnet', r'distilbert'\n",
    "        ],\n",
    "        'word_embeddings': [\n",
    "            r'embedding', r'word2vec', r'glove', r'fasttext', r'word vector',\n",
    "            r'distributed representation', r'semantic vector'\n",
    "        ],\n",
    "        'tensorflow_keras': [r'tensorflow', r'keras', r'tf\\.', r'tf2'],\n",
    "        'pytorch': [r'pytorch', r'torch', r'nn\\.'],\n",
    "        'gensim': [r'gensim'],\n",
    "        'stanford_nlp': [r'stanford', r'corenlp', r'stanford parser'],\n",
    "        'openai': [r'openai', r'gpt-3', r'chatgpt', r'davinci']\n",
    "    }\n",
    "    \n",
    "    # Apply categorization rules to each post\n",
    "    categorized_count = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processed {idx} posts...\")\n",
    "        \n",
    "        # Get title and content\n",
    "        title = str(row['title']).lower() if pd.notna(row['title']) else \"\"\n",
    "        body = str(row['body']).lower() if pd.notna(row['body']) else \"\"\n",
    "        \n",
    "        # Combine text for categorization\n",
    "        text = f\"{title} {body}\"\n",
    "        \n",
    "        # 1. Check if it's an implementation issue\n",
    "        for pattern in implementation_patterns:\n",
    "            if re.search(pattern, text):\n",
    "                df.at[idx, 'category'] = 'implementation_issues'\n",
    "                df.at[idx, 'classification_reason'] = f\"Matched implementation pattern: {pattern}\"\n",
    "                \n",
    "                for task, task_patterns in nlp_tasks.items():\n",
    "                    for task_pattern in task_patterns:\n",
    "                        if re.search(task_pattern, text):\n",
    "                            df.at[idx, 'subcategory'] = task\n",
    "                            break\n",
    "                    if pd.notna(df.at[idx, 'subcategory']):\n",
    "                        break\n",
    "                        \n",
    "                if pd.isna(df.at[idx, 'subcategory']):\n",
    "                    for tech, tech_patterns in technologies.items():\n",
    "                        for tech_pattern in tech_patterns:\n",
    "                            if re.search(tech_pattern, text):\n",
    "                                df.at[idx, 'subcategory'] = tech\n",
    "                                break\n",
    "                        if pd.notna(df.at[idx, 'subcategory']):\n",
    "                            break\n",
    "                \n",
    "                categorized_count += 1\n",
    "                break\n",
    "        \n",
    "        # 2. If not an implementation issue, check if it's an understanding issue\n",
    "        if pd.isna(df.at[idx, 'category']):\n",
    "            for pattern in understanding_patterns:\n",
    "                if re.search(pattern, text):\n",
    "                    df.at[idx, 'category'] = 'understanding_issues'\n",
    "                    df.at[idx, 'classification_reason'] = f\"Matched understanding pattern: {pattern}\"\n",
    "                    \n",
    "                    # Check if there's a subcategory (task or technology)\n",
    "                    for task, task_patterns in nlp_tasks.items():\n",
    "                        for task_pattern in task_patterns:\n",
    "                            if re.search(task_pattern, text):\n",
    "                                df.at[idx, 'subcategory'] = task\n",
    "                                break\n",
    "                        if pd.notna(df.at[idx, 'subcategory']):\n",
    "                            break\n",
    "                            \n",
    "                    if pd.isna(df.at[idx, 'subcategory']):\n",
    "                        for tech, tech_patterns in technologies.items():\n",
    "                            for tech_pattern in tech_patterns:\n",
    "                                if re.search(tech_pattern, text):\n",
    "                                    df.at[idx, 'subcategory'] = tech\n",
    "                                    break\n",
    "                            if pd.notna(df.at[idx, 'subcategory']):\n",
    "                                break\n",
    "                    \n",
    "                    categorized_count += 1\n",
    "                    break\n",
    "        \n",
    "        # 3. If none of the above, check task-related categories\n",
    "        if pd.isna(df.at[idx, 'category']):\n",
    "            for task, patterns in nlp_tasks.items():\n",
    "                for pattern in patterns:\n",
    "                    if re.search(pattern, text):\n",
    "                        df.at[idx, 'category'] = 'nlp_task'\n",
    "                        df.at[idx, 'subcategory'] = task\n",
    "                        df.at[idx, 'classification_reason'] = f\"Matched NLP task: {task}, pattern: {pattern}\"\n",
    "                        categorized_count += 1\n",
    "                        break\n",
    "                if pd.notna(df.at[idx, 'category']):\n",
    "                    break\n",
    "        \n",
    "        # 4. If none of the above, check technology-related categories\n",
    "        if pd.isna(df.at[idx, 'category']):\n",
    "            for tech, patterns in technologies.items():\n",
    "                for pattern in patterns:\n",
    "                    if re.search(pattern, text):\n",
    "                        df.at[idx, 'category'] = 'nlp_technology'\n",
    "                        df.at[idx, 'subcategory'] = tech\n",
    "                        df.at[idx, 'classification_reason'] = f\"Matched technology term: {tech}, pattern: {pattern}\"\n",
    "                        categorized_count += 1\n",
    "                        break\n",
    "                if pd.notna(df.at[idx, 'category']):\n",
    "                    break\n",
    "    \n",
    "    print(f\"Categorization complete! Categorized a total of {categorized_count} posts\")\n",
    "    \n",
    "    if categorized_count < 100:\n",
    "        print(f\"Warning: Only {categorized_count} posts were categorized, less than the required 100\")\n",
    "    \n",
    "    # Count categories\n",
    "    category_counts = df['category'].value_counts()\n",
    "    subcategory_counts = df['subcategory'].value_counts()\n",
    "    \n",
    "    print(\"\\nCategory distribution:\")\n",
    "    print(category_counts)\n",
    "    print(\"\\nSubcategory distribution:\")\n",
    "    print(subcategory_counts)\n",
    "    \n",
    "    categories_under_10 = [cat for cat, count in category_counts.items() if count < 10 and pd.notna(cat)]\n",
    "    if categories_under_10:\n",
    "        print(f\"\\nWarning: The following categories contain fewer than 10 posts: {categories_under_10}\")\n",
    "    \n",
    "    # Save categorization results\n",
    "    categorized_df = df[~df['category'].isna()]\n",
    "    categorized_df.to_csv(os.path.join(output_folder, 'nlp_posts_categorized.csv'), index=False)\n",
    "    print(f\"Categorization results saved to '{os.path.join(output_folder, 'nlp_posts_categorized.csv')}'\")\n",
    "    \n",
    "    # Create categorization visualizations\n",
    "    # 1. Main categories pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    category_counts.plot.pie(autopct='%1.1f%%', startangle=90, fontsize=12, figsize=(10, 8))\n",
    "    plt.title('Distribution of Main Categories for NLP Posts', fontsize=16)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'category_distribution_pie.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Subcategories bar chart\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    subcategory_counts.plot.barh(fontsize=12, figsize=(12, 10))\n",
    "    plt.title('Distribution of Subcategories for NLP Posts', fontsize=16)\n",
    "    plt.xlabel('Number of Posts', fontsize=12)\n",
    "    plt.ylabel('Subcategory', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'subcategory_distribution_bar.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Save categorization rules for reference\n",
    "    classification_rules = {\n",
    "        'implementation_patterns': implementation_patterns,\n",
    "        'understanding_patterns': understanding_patterns,\n",
    "        'nlp_tasks': nlp_tasks,\n",
    "        'technologies': technologies\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_folder, 'classification_rules.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(classification_rules, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Classification rules saved to '{os.path.join(output_folder, 'classification_rules.json')}'\")\n",
    "    print(\"Post categorization complete!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    categorized_df = categorize_posts(\"nlp_stackoverflow_posts_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a603bcd-cddf-4164-a46f-447ee753d806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
